{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting an Election from Tweets     \n",
    "\n",
    "[MichaÃ«l Juillard](michael.juillard@epfl.ch), \n",
    "[Mikhail Vorobiev](mikhail.vorobiev@epfl.ch),\n",
    "[Chiara Ercolani](chiara.ercolani@epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Problem Definition\n",
    "\n",
    "Nowadays social medias like Twitter and Facebook are means by which people continuously express their opinion on any matter. Thus, data mining combined with data analysis could be a great way to undersand the general feeling of the users on a certian matter.\n",
    "With this in mind, we decided to try to predict the result of an election using data from Twitter.\n",
    "\n",
    "We focused on the US Senate Election of 2016, since this election would provide enough meaningful data for our analysis. In fact there are many candidates and a lot of people are tweeting about them, allowing us to have a big dataset to train our algorithm with. We started by mining data from Twitter, collecting every Tweet regarding every Rupublican and Democrate Senate candidate during the two months preceeding the elections. We collected tweets aimed at the candidate (@candidateprofile) or that contained an hashtag referred to the candidate (#candidateprofile). \n",
    "\n",
    "The second step was to perform a sentiment analysis on these tweets to understand if the user was expressing a positive or negative feeling towards the candidate. Sentiment analysis combines natural language processing, text analysis and computational linguistics to assess the attitude of a writer towards a topic. There are various tools for it, we picked one called Pattern developed by the University of Antwerp, in Belgium. \n",
    "\n",
    "Afterwards we trained a machine learning algorithm with the sentiment analysis data. TODO describe this shit\n",
    "\n",
    "Finally we used other sentiment analysis data from the same election and analyzed whether our algorithm was able to predict the election result or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Resources\n",
    "\n",
    "For the project we used a variety of tools, here are the links to their websites.\n",
    "\n",
    "Links for used for Twitter data mining\n",
    "* [US Senate Elections 2016 Wikipedia page](https://en.wikipedia.org/wiki/United_States_Senate_elections,_2016)\n",
    "* [Twitter REST API](https://dev.twitter.com/rest/public)\n",
    "* [Tool to get older Tweets](https://github.com/Jefferson-Henrique/GetOldTweets-python)\n",
    "\n",
    "\n",
    "Tool used for the sentiment analysis\n",
    "* [Sentiment Analysis Tool](http://www.clips.ua.ac.be/pattern)\n",
    "\n",
    "Papers about election prediction with tweets\n",
    "* [Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment](https://www.google.ch/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjqp9O79qPRAhWCvhQKHb-SAq4QFggcMAA&url=http%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FICWSM%2FICWSM10%2Fpaper%2Fdownload%2F1441%2F1852&usg=AFQjCNFnJCLUH96xhdmhtDbDnJs4Dtx8jg)\n",
    "* [Limits of Electoral Predictions Using Twitter](http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2862/3254)\n",
    "* [On Using Twitter to Monitor Political Sentimentand Predict Election Results](https://www.aclweb.org/anthology/W/W11/W11-3702.pdf)\n",
    "* [Predicting US Primary Elections with Twitter](http://snap.stanford.edu/social2012/papers/shi.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Web scraping\n",
    "\n",
    "We started by finding the list of candidates for the US Senate Elections in 2016 and we looked for their twitter accounts. We decided to only use Democratic and Republican candidates, as the parties are not very relevant in the US. \n",
    "\n",
    "Twitter APIs only allow data mining in the past week, thus we used a tool to bypass this limitation and collect data from the 8th of September 2016 to the 8th of November 2016, the day of the elections.\n",
    "\n",
    "The outputs of this analysis are kept in the *data* folder of our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is used to measure the polarity of a text. By polarity it is intended wheter the text leans to a negative or positive attitute towards the topic it contains. \n",
    "\n",
    "Patter, used to perform sentiment analysis for this project, is a natural language processing toolkit. In particular, we used pattern.en, which was made for the English language. \n",
    "\n",
    "The **sentiment( )** function returns a **(polarity, subjectivity)**-tuple for the given sentence, based on the adjectives it contains, where polarity is a value between -1.0 and +1.0 and subjectivity between 0.0 and 1.0. \n",
    "\n",
    "**Important note** : this sentiment analysis tool requires the usage of Python 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_got#barackobama @barackobama.csv\n",
      "output_gotCampbellforLa.csv\n",
      "[ 0.  0.]\n",
      "26\n",
      "output_gotCatherineForNV.csv\n",
      "[ 0.  0.]\n",
      "4107\n",
      "[ 0.  0.]\n",
      "5155\n",
      "[ 0.  0.]\n",
      "5295\n",
      "[ 0.  0.]\n",
      "6353\n",
      "output_gotChrisvance123.csv\n",
      "output_gotChrisVanHollen.csv\n",
      "output_gotChuckGrassley.csv\n",
      "[ 0.  0.]\n",
      "64\n",
      "[ 0.  0.]\n",
      "2787\n",
      "[ 0.  0.]\n",
      "3762\n",
      "[ 0.  0.]\n",
      "5977\n",
      "output_gotDanCarterCT.csv\n",
      "output_gotGovernorHassan.csv\n",
      "output_gotJasonKander.csv\n",
      "[ 0.  0.]\n",
      "1055\n",
      "output_gotJerryMoran.csv\n",
      "output_gotjimbarksdale.csv\n",
      "[ 0.  0.]\n",
      "103\n",
      "output_gotJimGrayLexKY.csv\n",
      "[ 0.  0.]\n",
      "253\n",
      "output_gotJohnKennedyLA.csv\n",
      "[ 0.  0.]\n",
      "333\n",
      "[ 0.  0.]\n",
      "334\n",
      "output_gotKamalaHarris.csv\n",
      "[ 0.  0.]\n",
      "4977\n",
      "output_gotKathyforMD.csv\n",
      "[ 0.  0.]\n",
      "848\n",
      "output_gotKellyAyotte.csv\n",
      "[ 0.  0.]\n",
      "1009\n",
      "[ 0.  0.]\n",
      "1718\n",
      "[ 0.  0.]\n",
      "1967\n",
      "[ 0.  0.]\n",
      "3091\n",
      "[ 0.  0.]\n",
      "3593\n",
      "[ 0.  0.]\n",
      "5837\n",
      "[ 0.  0.]\n",
      "8457\n",
      "[ 0.  0.]\n",
      "8476\n",
      "[ 0.  0.]\n",
      "8626\n",
      "[ 0.  0.]\n",
      "12289\n",
      "[ 0.  0.]\n",
      "17046\n",
      "[ 0.  0.]\n",
      "17685\n",
      "[ 0.  0.]\n",
      "18549\n",
      "[ 0.  0.]\n",
      "19804\n",
      "[ 0.  0.]\n",
      "20081\n",
      "[ 0.  0.]\n",
      "20086\n",
      "[ 0.  0.]\n",
      "22323\n",
      "[ 0.  0.]\n",
      "23604\n",
      "[ 0.  0.]\n",
      "23817\n",
      "[ 0.  0.]\n",
      "24799\n",
      "output_gotLorettaSanchez.csv\n",
      "[ 0.  0.]\n",
      "19\n",
      "[ 0.  0.]\n",
      "863\n",
      "output_gotmarcorubio.csv\n",
      "[ 0.  0.]\n",
      "729\n",
      "[ 0.  0.]\n",
      "1120\n",
      "[ 0.  0.]\n",
      "2721\n",
      "[ 0.  0.]\n",
      "7987\n",
      "[ 0.  0.]\n",
      "8685\n",
      "[ 0.  0.]\n",
      "12101\n",
      "[ 0.  0.]\n",
      "18227\n",
      "[ 0.  0.]\n",
      "23664\n",
      "[ 0.  0.]\n",
      "26657\n",
      "[ 0.  0.]\n",
      "33836\n",
      "[ 0.  0.]\n",
      "35375\n",
      "[ 0.  0.]\n",
      "43013\n",
      "[ 0.  0.]\n",
      "43673\n",
      "[ 0.  0.]\n",
      "44418\n",
      "[ 0.  0.]\n",
      "55500\n",
      "[ 0.  0.]\n",
      "60601\n",
      "[ 0.  0.]\n",
      "68143\n",
      "output_gotMikeCrapo.csv\n",
      "[ 0.  0.]\n",
      "1747\n",
      "[ 0.  0.]\n",
      "2838\n",
      "output_gotPatrickMurphyFL.csv\n",
      "[ 0.  0.]\n",
      "2922\n",
      "[ 0.  0.]\n",
      "5174\n",
      "[ 0.  0.]\n",
      "7490\n",
      "[ 0.  0.]\n",
      "9400\n",
      "[ 0.  0.]\n",
      "12837\n",
      "[ 0.  0.]\n",
      "14568\n",
      "[ 0.  0.]\n",
      "15571\n",
      "[ 0.  0.]\n",
      "19855\n",
      "[ 0.  0.]\n",
      "21513\n",
      "output_gotpattyforiowa.csv\n",
      "[ 0.  0.]\n",
      "764\n",
      "output_gotPattyMurray.csv\n",
      "[ 0.  0.]\n",
      "1121\n",
      "[ 0.  0.]\n",
      "2582\n",
      "output_gotRandPaul.csv\n",
      "[ 0.  0.]\n",
      "408\n",
      "[ 0.  0.]\n",
      "1123\n",
      "[ 0.  0.]\n",
      "2109\n",
      "[ 0.  0.]\n",
      "3049\n",
      "[ 0.  0.]\n",
      "11018\n",
      "[ 0.  0.]\n",
      "11089\n",
      "[ 0.  0.]\n",
      "11280\n",
      "[ 0.  0.]\n",
      "11960\n",
      "[ 0.  0.]\n",
      "17039\n",
      "[ 0.  0.]\n",
      "22232\n",
      "[ 0.  0.]\n",
      "24643\n",
      "output_gotRepJoeHeck.csv\n",
      "[ 0.  0.]\n",
      "457\n",
      "output_gotRepKirkpatrick.csv\n",
      "[ 0.  0.]\n",
      "446\n",
      "output_gotRickSantorum.csv\n",
      "[ 0.  0.]\n",
      "537\n",
      "[ 0.  0.]\n",
      "2322\n",
      "[ 0.  0.]\n",
      "2475\n",
      "output_gotRonWyden.csv\n",
      "output_gotRoyBlunt.csv\n",
      "[ 0.  0.]\n",
      "2278\n",
      "output_gotSenatorIsakson.csv\n",
      "output_gotSenatorKirk.csv\n",
      "[ 0.  0.]\n",
      "1797\n",
      "[ 0.  0.]\n",
      "2332\n",
      "[ 0.  0.]\n",
      "3549\n",
      "[ 0.  0.]\n",
      "4245\n",
      "output_gotSenBlumenthal.csv\n",
      "output_gotSenEvanBayh.csv\n",
      "[ 0.  0.]\n",
      "265\n",
      "output_gotSenJohnMcCain.csv\n",
      "[ 0.  0.]\n",
      "2465\n",
      "[ 0.  0.]\n",
      "2870\n",
      "[ 0.  0.]\n",
      "6256\n",
      "[ 0.  0.]\n",
      "9230\n",
      "[ 0.  0.]\n",
      "15387\n",
      "[ 0.  0.]\n",
      "18204\n",
      "[ 0.  0.]\n",
      "23388\n",
      "[ 0.  0.]\n",
      "24349\n",
      "[ 0.  0.]\n",
      "25953\n",
      "[ 0.  0.]\n",
      "26121\n",
      "[ 0.  0.]\n",
      "26480\n",
      "output_gotsenrobportman.csv\n",
      "[ 0.  0.]\n",
      "1849\n",
      "[ 0.  0.]\n",
      "2304\n",
      "[ 0.  0.]\n",
      "3687\n",
      "output_gotSenSchumer.csv\n",
      "output_gotSturgill4Idaho.csv\n",
      "output_gotTammyforIL.csv\n",
      "[ 0.  0.]\n",
      "2908\n",
      "[ 0.  0.]\n",
      "4426\n",
      "output_gotTed_Strickland.csv\n",
      "[ 0.  0.]\n",
      "3387\n",
      "[ 0.  0.]\n",
      "4125\n",
      "[ 0.  0.]\n",
      "4375\n",
      "[ 0.  0.]\n",
      "5643\n",
      "output_gotToddYoungIN.csv\n",
      "[ 0.  0.]\n",
      "355\n",
      "[ 0.  0.]\n",
      "2478\n",
      "output_gotWendyLongNY.csv\n",
      "output_gotwiesner4senate.csv\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import sentiment\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "for file in os.listdir(\"./data\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        path = r'data/'+file\n",
    "        print(file)\n",
    "        var = os.path.basename(path)\n",
    "        var= str.split(var,'_')\n",
    "        var=str.split(var[1],'.')\n",
    "        sentiment_path = 'sentiment_analysis_v1/sentiment_'+var[0]+'.csv'\n",
    "\n",
    "        text=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(4,))\n",
    "        date=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(1,))\n",
    "        retweet=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(2,))\n",
    "        favourites=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(3,))\n",
    "\n",
    "        ## remove hashtags and @ from the tweets \n",
    "        text=np.core.defchararray.replace(text,'#',' ')\n",
    "        text=np.core.defchararray.replace(text,'@',' ')\n",
    "\n",
    "        array_sentiment=np.zeros((len(text),2))\n",
    "\n",
    "\n",
    "        # perform sentiment analysis \n",
    "        for i in range(len(text)):\n",
    "            array_sentiment[i] = sentiment(text[i])\n",
    "            if (array_sentiment[i][0]<0.0000000000000001 and array_sentiment[i][0]> -0.0000000000000001 and array_sentiment[i][0]!=0.0):\n",
    "                array_sentiment[i]=0\n",
    "                \n",
    "\n",
    "        # save sentiment analysis to output file\n",
    "        np.savetxt(sentiment_path,np.transpose([date,array_sentiment[:,0],array_sentiment[:,1],retweet,favourites]),fmt=\"%s;%s;%s;%s;%s\",delimiter=';',header=\"date;sentiment;objectivity;retweets;favourites\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Unique User Identification\n",
    "\n",
    "We decided to identify the number of unique users who tweeted about a certain candidate and use this number as a feature for our machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import os\n",
    "for file in os.listdir(\"./data\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        cand = r'data/'+file\n",
    "        filecp = codecs.open(cand, encoding = 'latin-1')\n",
    "        mydata = np.loadtxt(filecp, dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(0,))\n",
    "        print(file + \" \" + str(len(mydata)) + \" \" + str(len(np.unique(mydata))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.3 Mean \n",
    "\n",
    "We decided to do a daily mean of the polarity value given by the sentiment analysis of the tweets. The mean is weighted on the number of likes and retweets that every tweet received. In fact we assumed that likes and retweets meant that people agreed with the content of the tweet and thus such tweets deserved to have a higher weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_got#barackobama @barackobama.csv\n",
      "sentiment_gotCampbellforLa.csv\n",
      "sentiment_gotCatherineForNV.csv\n",
      "sentiment_gotChrisvance123.csv\n",
      "sentiment_gotChrisVanHollen.csv\n",
      "sentiment_gotChuckGrassley.csv\n",
      "sentiment_gotDanCarterCT.csv\n",
      "sentiment_gotGovernorHassan.csv\n",
      "sentiment_gotJasonKander.csv\n",
      "sentiment_gotJerryMoran.csv\n",
      "sentiment_gotjimbarksdale.csv\n",
      "sentiment_gotJimGrayLexKY.csv\n",
      "sentiment_gotJohnKennedyLA.csv\n",
      "sentiment_gotKamalaHarris.csv\n",
      "sentiment_gotKathyforMD.csv\n",
      "sentiment_gotKellyAyotte.csv\n",
      "sentiment_gotLorettaSanchez.csv\n",
      "sentiment_gotmarcorubio.csv\n",
      "sentiment_gotMikeCrapo.csv\n",
      "sentiment_gotPatrickMurphyFL.csv\n",
      "sentiment_gotpattyforiowa.csv\n",
      "sentiment_gotPattyMurray.csv\n",
      "sentiment_gotRandPaul.csv\n",
      "sentiment_gotRepJoeHeck.csv\n",
      "sentiment_gotRepKirkpatrick.csv\n",
      "sentiment_gotRickSantorum.csv\n",
      "sentiment_gotRonWyden.csv\n",
      "sentiment_gotRoyBlunt.csv\n",
      "sentiment_gotSenatorIsakson.csv\n",
      "sentiment_gotSenatorKirk.csv\n",
      "sentiment_gotSenBlumenthal.csv\n",
      "sentiment_gotSenEvanBayh.csv\n",
      "sentiment_gotSenJohnMcCain.csv\n",
      "sentiment_gotsenrobportman.csv\n",
      "sentiment_gotSenSchumer.csv\n",
      "sentiment_gotSturgill4Idaho.csv\n",
      "sentiment_gotTammyforIL.csv\n",
      "sentiment_gotTed.csv\n",
      "sentiment_gotToddYoungIN.csv\n",
      "sentiment_gotWendyLongNY.csv\n",
      "sentiment_gotwiesner4senate.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "\n",
    "for file in os.listdir(\"./sentiment_analysis_v1\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        path = r'sentiment_analysis_v1/'+file\n",
    "        print(file)\n",
    "        var = os.path.basename(path)\n",
    "        var= str.split(var,'_')\n",
    "        var=str.split(var[1],'.')\n",
    "        mean_path = 'mean_v1/mean_'+var[0]+'.csv'\n",
    "        sentim=np.loadtxt(path, comments='++++',delimiter=';',skiprows=1,usecols=(1,))\n",
    "        date=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(0,))\n",
    "        retweet=np.loadtxt(path, comments='++++',delimiter=';',skiprows=1,usecols=(3,))\n",
    "        favourites=np.loadtxt(path, comments='++++',delimiter=';',skiprows=1,usecols=(4,))\n",
    "\n",
    "        array_length=62\n",
    "        mean=[0]*array_length\n",
    "\n",
    "\n",
    "        #create day and month arrays\n",
    "        array_day=np.append(np.arange(8,0,-1),(np.append(np.arange(31,0,-1),np.arange(30,7,-1))))\n",
    "        array_month=np.append(np.ones(8)*11,(np.append(np.ones(31)*10,np.ones(23)*9)))\n",
    "\n",
    "        # parse the date and the month and create arrays for them\n",
    "        day =np.zeros(len(date))\n",
    "        month=np.zeros(len(date))\n",
    "        for i in range(len(date)):\n",
    "            day[i]=np.datetime64(date[i]).astype(object).day\n",
    "            month[i]=np.datetime64(date[i]).astype(object).month\n",
    "\n",
    "\n",
    "        #create array of the weights (based on likes and favourites)\n",
    "        array_weight = np.zeros(len(sentim))\n",
    "        for i in range(len(retweet)):\n",
    "            if(retweet[i]!=0.0 or favourites[i]!=0.0):\n",
    "                array_weight[i]=retweet[i]+favourites[i]\n",
    "            else:\n",
    "                array_weight[i]= 1\n",
    "\n",
    "        #compute the mean\n",
    "        cnt_date=0\n",
    "        cnt_mean=[0]*array_length\n",
    "\n",
    "        for i in range(len(sentim)):\n",
    "            if (sentim[i]!=0.0):\n",
    "                if (array_day[cnt_date]==day[i]and array_month[cnt_date]==month[i]): \n",
    "                        mean[cnt_date] = mean[cnt_date]+array_weight[i]*sentim[i]\n",
    "                        cnt_mean[cnt_date]=cnt_mean[cnt_date]+array_weight[i]\n",
    "\n",
    "                else :\n",
    "                    \n",
    "                    while (array_day[cnt_date]!=day[i] or array_month[cnt_date]!= month[i]): \n",
    "                        if (cnt_date <len(array_day)-1):\n",
    "                            cnt_date=cnt_date + 1\n",
    "                        else:\n",
    "                            break\n",
    "                        \n",
    "                    mean[cnt_date]=mean[cnt_date]+array_weight[i]*sentim[i]\n",
    "                    cnt_mean[cnt_date]=cnt_mean[cnt_date]+array_weight[i]\n",
    "\n",
    "\n",
    "\n",
    "        weigthed_mean=[None]*array_length\n",
    "\n",
    "        for i in range(len(mean)):\n",
    "            if (mean[i]!=0.0):\n",
    "                weigthed_mean[i]=mean[i]/cnt_mean[i]\n",
    "            else:\n",
    "                weigthed_mean[i]=0\n",
    "                \n",
    "        # save output on file\n",
    "        np.savetxt(mean_path,np.transpose([array_day,array_month,mean,cnt_mean,weigthed_mean]),fmt=\"%d;%d;%s;%s;%s\",delimiter=';',header=\"day;month;sum;weight;mean\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Machine Learning Algorithm\n",
    "\n",
    "We used a linear regression algorithm with multiple outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Prediction Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Conclusions\n",
    "\n",
    "Sentiment Analysis, like any other natural language processing tool, is hard to perform and does not give extremely accurate results. Performing it on Tweets is tricky because many slang words are used and often the analysis does not output anything. We were aware of this but saw that with a big dataset we were able to have a reasonable number of tweets that could be processed by the sentiment analysis and give accurate results.\n",
    "\n",
    "Another issue that we encountered was that the number of tweets differed a lot depending on the popularity of the candidate. However, since we looked at big elections in a big country, we were able to have a quite big dataset anyways.\n",
    "\n",
    "used one election to keep same condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_gotCatherineForNV.csv\n",
      "[ 0.  0.]\n",
      "4107\n",
      "[ 0.  0.]\n",
      "5155\n",
      "[ 0.  0.]\n",
      "5295\n",
      "[ 0.  0.]\n",
      "6353\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import sentiment\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "path = r'data/output_gotCatherineForNV.csv'\n",
    "print(file)\n",
    "var = os.path.basename(path)\n",
    "var= str.split(var,'_')\n",
    "var=str.split(var[1],'.')\n",
    "sentiment_path = 'sentiment_analysis_v1/sentiment_'+var[0]+'.csv'\n",
    "\n",
    "text=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(4,))\n",
    "date=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(1,))\n",
    "retweet=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(2,))\n",
    "favourites=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(3,))\n",
    "\n",
    "## remove hashtags and @ from the tweets \n",
    "text=np.core.defchararray.replace(text,'#',' ')\n",
    "text=np.core.defchararray.replace(text,'@',' ')\n",
    "\n",
    "array_sentiment=np.zeros((len(text),2))\n",
    "\n",
    "\n",
    "# perform sentiment analysis \n",
    "for i in range(len(text)):\n",
    "    array_sentiment[i] = sentiment(text[i])\n",
    "    if (array_sentiment[i][0]<0.0000000000000001 and array_sentiment[i][0]> -0.0000000000000001 and array_sentiment[i][0]!=0.0):\n",
    "        array_sentiment[i]=0\n",
    "        print(array_sentiment[i])\n",
    "        print(i)\n",
    "\n",
    "        \n",
    "        \n",
    "# save sentiment analysis to output file\n",
    "np.savetxt(sentiment_path,np.transpose([date,array_sentiment[:,0],array_sentiment[:,1],retweet,favourites]),fmt=\"%s;%s;%s;%s;%s\",delimiter=';',header=\"date;sentiment;objectivity;retweets;favourites\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
