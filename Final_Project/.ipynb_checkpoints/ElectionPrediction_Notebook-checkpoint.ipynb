{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting an Election from Tweets     \n",
    "\n",
    "[Michaël Juillard](michael.juillard@epfl.ch), \n",
    "[Mikhail Vorobiev](mikhail.vorobiev@epfl.ch),\n",
    "[Chiara Ercolani](chiara.ercolani@epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Problem Definition\n",
    "\n",
    "Nowadays social medias like Twitter and Facebook are means by which people continuously express their opinion on any matter. Thus, data mining combined with data analysis could be a great way to undersand the general feeling of the users on a certian matter.\n",
    "With this in mind, we decided to try to predict the result of an election using data from Twitter.\n",
    "\n",
    "We focused on the US Senate Election of 2016, since this election would provide enough meaningful data for our analysis. In fact there are many candidates and a lot of people are tweeting about them, allowing us to have a big dataset to train our algorithm with. We started by mining data from Twitter, collecting every Tweet regarding most of the Republican and Democrate Senate candidates during the two months preceeding the elections. We collected tweets aimed at each candidate, meaning tweets containing the candidate's twitter name . \n",
    "\n",
    "The second step was to perform a sentiment analysis on these tweets to understand if the user was expressing a positive or negative feeling towards the candidate. Sentiment analysis combines natural language processing, text analysis and computational linguistics to assess the attitude of a writer towards a topic. There are various tools for it, we picked one called Pattern developed by the University of Antwerp, in Belgium. \n",
    "\n",
    "Afterwards we trained two variations of the same machine learning algorithm with the sentiment analysis data and some other features like number of followers of the candidate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Resources\n",
    "\n",
    "For the project we used a variety of tools, here are the links to their websites.\n",
    "\n",
    "Links used for Twitter data mining\n",
    "* [US Senate Elections 2016 Wikipedia page](https://en.wikipedia.org/wiki/United_States_Senate_elections,_2016)\n",
    "* [Twitter REST API](https://dev.twitter.com/rest/public)\n",
    "* [Tool to get older Tweets](https://github.com/Jefferson-Henrique/GetOldTweets-python)\n",
    "\n",
    "\n",
    "Tool used for the sentiment analysis\n",
    "* [Sentiment Analysis Tool](http://www.clips.ua.ac.be/pattern)\n",
    "\n",
    "Papers about election prediction with tweets\n",
    "* [Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment](https://www.google.ch/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwjqp9O79qPRAhWCvhQKHb-SAq4QFggcMAA&url=http%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FICWSM%2FICWSM10%2Fpaper%2Fdownload%2F1441%2F1852&usg=AFQjCNFnJCLUH96xhdmhtDbDnJs4Dtx8jg)\n",
    "* [Limits of Electoral Predictions Using Twitter](http://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/viewFile/2862/3254)\n",
    "* [On Using Twitter to Monitor Political Sentimentand Predict Election Results](https://www.aclweb.org/anthology/W/W11/W11-3702.pdf)\n",
    "* [Predicting US Primary Elections with Twitter](http://snap.stanford.edu/social2012/papers/shi.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Web scraping\n",
    "\n",
    "We started by finding the list of candidates for the United States Senate Elections in 2016 and we looked for their twitter accounts. We decided to only use Democratic and Republican candidates, as the other parties are not very relevant in the US. \n",
    "\n",
    "Twitter APIs only allow data mining in the past week, thus we used a tool to bypass this limitation and collect data from the 8th of September 2016 to the 8th of November 2016, the day of the elections.\n",
    "\n",
    "The tool is a Python script found on GitHub (link in the *Resources* section). It enabled us to get all the tweets in a desired time frame according to some custom parameters. For example to mine tweets about Evan Bayh from the 8th of Septemebr 2016 to the 8th of November 2016 we simply ran the following command:\n",
    "\n",
    "*python Exporter.py --querysearch \"SenEvanBayh\" --since 2016-09-08 --until 2016-11-08*\n",
    "\n",
    "The outputs of this analysis are kept in the *data* folder of our project in .csv format. The files contain the date of the tweet, the username of the author of the tweet, the tweet itself, the number of likes and retweets for the tweet and some other information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Analysis\n",
    "\n",
    "This section will present the analysis that was performed on the data to make it usable for the machine learning part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imported libraries\n",
    "from pattern.en import sentiment\n",
    "import numpy as np \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is used to measure the polarity of a text. By polarity it is intended whether the text leans to a negative or positive attitute towards the topic it contains. \n",
    "\n",
    "Pattern, used to perform sentiment analysis for this project, is a natural language processing toolkit. In particular, we used pattern.en, which was made for the English language. \n",
    "\n",
    "The **sentiment( )** function returns a **(polarity, subjectivity)**-tuple for the given sentence, based on the adjectives it contains, where polarity is a value between -1.0 and +1.0 and subjectivity between 0.0 and 1.0. \n",
    "\n",
    "Before performing the sentiment analysis on the data, we got rid of the hashtags and of the @ to facilitate the sentiment analysis in case a meaningful word followed these symbols.\n",
    "\n",
    "**Important note** : this sentiment analysis tool requires the usage of Python 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_gotCampbellforLa.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pattern/text/__init__.py:1943: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  if w in imap(lambda e: e.lower(), e):\n",
      "pattern/text/__init__.py:979: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  and tokens[j] in (\"'\", \"\\\"\", u\"”\", u\"’\", \"...\", \".\", \"!\", \"?\", \")\", EOS):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_gotCatherineForNV.csv\n",
      "output_gotChrisvance123.csv\n",
      "output_gotChrisVanHollen.csv\n",
      "output_gotChuckGrassley.csv\n",
      "output_gotDanCarterCT.csv\n",
      "output_gotGovernorHassan.csv\n",
      "output_gotJasonKander.csv\n",
      "output_gotJerryMoran.csv\n",
      "output_gotjimbarksdale.csv\n",
      "output_gotJimGrayLexKY.csv\n",
      "output_gotJohnKennedyLA.csv\n",
      "output_gotKamalaHarris.csv\n",
      "output_gotKathyforMD.csv\n",
      "output_gotKellyAyotte.csv\n",
      "output_gotLorettaSanchez.csv\n",
      "output_gotmarcorubio.csv\n",
      "output_gotMikeCrapo.csv\n",
      "output_gotPatrickMurphyFL.csv\n",
      "output_gotpattyforiowa.csv\n",
      "output_gotPattyMurray.csv\n",
      "output_gotRandPaul.csv\n",
      "output_gotRepJoeHeck.csv\n",
      "output_gotRepKirkpatrick.csv\n",
      "output_gotRoyBlunt.csv\n",
      "output_gotSenatorIsakson.csv\n",
      "output_gotSenatorKirk.csv\n",
      "output_gotSenBlumenthal.csv\n",
      "output_gotSenEvanBayh.csv\n",
      "output_gotSenJohnMcCain.csv\n",
      "output_gotsenrobportman.csv\n",
      "output_gotSenSchumer.csv\n",
      "output_gotSturgill4Idaho.csv\n",
      "output_gotTammyforIL.csv\n",
      "output_gotTedStrickland.csv\n",
      "output_gotToddYoungIN.csv\n",
      "output_gotWendyLongNY.csv\n",
      "output_gotwiesner4senate.csv\n"
     ]
    }
   ],
   "source": [
    "# Use all the .csv files inside the data folder\n",
    "for file in os.listdir(\"./data\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        path = r'data/'+file\n",
    "        print(file)\n",
    "        var = os.path.basename(path)\n",
    "        var= str.split(var,'_')\n",
    "        var=str.split(var[1],'.')\n",
    "        sentiment_path = 'sentim/sentiment_'+var[0]+'.csv'\n",
    "\n",
    "        # read the text, the date and the number of retwetts and favourites from the data files\n",
    "        text=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(4,))\n",
    "        date=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(1,))\n",
    "        retweet=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(2,))\n",
    "        favourites=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(3,))\n",
    "\n",
    "        # remove hashtags and @ from the tweets \n",
    "        text=np.core.defchararray.replace(text,'#',' ')\n",
    "        text=np.core.defchararray.replace(text,'@',' ')\n",
    "\n",
    "        array_sentiment=np.zeros((len(text),2))\n",
    "\n",
    "\n",
    "        # perform sentiment analysis and filter out values that are too close to zero\n",
    "        for i in range(len(text)):\n",
    "            array_sentiment[i] = sentiment(text[i])\n",
    "            if (array_sentiment[i][0]<0.0000000000000001 and array_sentiment[i][0]> -0.0000000000000001 and array_sentiment[i][0]!=0.0):\n",
    "                array_sentiment[i]=0\n",
    "                \n",
    "\n",
    "        # save sentiment analysis to output file\n",
    "        np.savetxt(sentiment_path,np.transpose([date,array_sentiment[:,0],array_sentiment[:,1],retweet,favourites]),fmt=\"%s;%s;%s;%s;%s\",delimiter=';',header=\"date;sentiment;objectivity;retweets;favourites\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Unique User Identification\n",
    "\n",
    "We decided to identify the number of unique users who tweeted about a certain candidate and use this number as a feature for our machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b9c923f21ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mcand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmydata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'++++'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"./data\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        cand = r'data/'+file\n",
    "        mydata = np.loadtxt(cand, dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(0,))\n",
    "        print(file + \" \" + str(len(mydata)) + \" \" + str(len(np.unique(mydata))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.3 Mean \n",
    "\n",
    "We decided to do a daily mean of the polarity value given by the sentiment analysis of the tweets. The mean is weighted on the number of likes and retweets that every tweet received. In fact we assumed that likes and retweets meant that people agreed with the content of the tweet and thus such tweets deserved to have a higher weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_gotCampbellforLa.csv\n",
      "sentiment_gotCatherineForNV.csv\n",
      "sentiment_gotChrisvance123.csv\n",
      "sentiment_gotChrisVanHollen.csv\n",
      "sentiment_gotChuckGrassley.csv\n",
      "sentiment_gotDanCarterCT.csv\n",
      "sentiment_gotGovernorHassan.csv\n",
      "sentiment_gotJasonKander.csv\n",
      "sentiment_gotJerryMoran.csv\n",
      "sentiment_gotjimbarksdale.csv\n",
      "sentiment_gotJimGrayLexKY.csv\n",
      "sentiment_gotJohnKennedyLA.csv\n",
      "sentiment_gotKamalaHarris.csv\n",
      "sentiment_gotKathyforMD.csv\n",
      "sentiment_gotKellyAyotte.csv\n",
      "sentiment_gotLorettaSanchez.csv\n",
      "sentiment_gotmarcorubio.csv\n",
      "sentiment_gotMikeCrapo.csv\n",
      "sentiment_gotPatrickMurphyFL.csv\n",
      "sentiment_gotpattyforiowa.csv\n",
      "sentiment_gotPattyMurray.csv\n",
      "sentiment_gotRandPaul.csv\n",
      "sentiment_gotRepJoeHeck.csv\n",
      "sentiment_gotRepKirkpatrick.csv\n",
      "sentiment_gotRoyBlunt.csv\n",
      "sentiment_gotSenatorIsakson.csv\n",
      "sentiment_gotSenatorKirk.csv\n",
      "sentiment_gotSenBlumenthal.csv\n",
      "sentiment_gotSenEvanBayh.csv\n",
      "sentiment_gotSenJohnMcCain.csv\n",
      "sentiment_gotsenrobportman.csv\n",
      "sentiment_gotSenSchumer.csv\n",
      "sentiment_gotSturgill4Idaho.csv\n",
      "sentiment_gotTammyforIL.csv\n",
      "sentiment_gotTedStrickland.csv\n",
      "sentiment_gotToddYoungIN.csv\n",
      "sentiment_gotWendyLongNY.csv\n",
      "sentiment_gotwiesner4senate.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"./sentim\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        path = r'sentim/'+file\n",
    "        print(file)\n",
    "        var = os.path.basename(path)\n",
    "        var= str.split(var,'_')\n",
    "        var=str.split(var[1],'.')\n",
    "        mean_path = 'means/mean_'+var[0]+'.csv'\n",
    "        \n",
    "        #read sentiment, date and number of retweets and favourites from the sentiment analysis files\n",
    "        sentim=np.loadtxt(path, comments='++++',delimiter=';',skiprows=1,usecols=(1,))\n",
    "        date=np.loadtxt(path,dtype=str, comments='++++',delimiter=';',skiprows=1,usecols=(0,))\n",
    "        retweet=np.loadtxt(path, comments='++++',delimiter=';',skiprows=1,usecols=(3,))\n",
    "        favourites=np.loadtxt(path, comments='++++',delimiter=';',skiprows=1,usecols=(4,))\n",
    "\n",
    "        array_length=62\n",
    "        mean=[0]*array_length\n",
    "\n",
    "\n",
    "        #create day and month arrays\n",
    "        array_day=np.append(np.arange(8,0,-1),(np.append(np.arange(31,0,-1),np.arange(30,7,-1))))\n",
    "        array_month=np.append(np.ones(8)*11,(np.append(np.ones(31)*10,np.ones(23)*9)))\n",
    "\n",
    "        # parse the date and the month and create arrays for them\n",
    "        day =np.zeros(len(date))\n",
    "        month=np.zeros(len(date))\n",
    "        for i in range(len(date)):\n",
    "            day[i]=np.datetime64(date[i]).astype(object).day\n",
    "            month[i]=np.datetime64(date[i]).astype(object).month\n",
    "\n",
    "\n",
    "        #create array of the weights (based on likes and favourites)\n",
    "        array_weight = np.zeros(len(sentim))\n",
    "        for i in range(len(retweet)):\n",
    "            if(retweet[i]!=0.0 or favourites[i]!=0.0):\n",
    "                array_weight[i]=retweet[i]+favourites[i]\n",
    "            else:\n",
    "                array_weight[i]= 1\n",
    "\n",
    "        #compute the mean\n",
    "        cnt_date=0\n",
    "        cnt_mean=[0]*array_length\n",
    "\n",
    "        for i in range(len(sentim)):\n",
    "            if (sentim[i]!=0.0):\n",
    "                if (array_day[cnt_date]==day[i]and array_month[cnt_date]==month[i]): \n",
    "                        mean[cnt_date] = mean[cnt_date]+array_weight[i]*sentim[i]\n",
    "                        cnt_mean[cnt_date]=cnt_mean[cnt_date]+array_weight[i]\n",
    "\n",
    "                else :\n",
    "                    \n",
    "                    while (array_day[cnt_date]!=day[i] or array_month[cnt_date]!= month[i]): \n",
    "                        if (cnt_date <len(array_day)-1):\n",
    "                            cnt_date=cnt_date + 1\n",
    "                        else:\n",
    "                            break\n",
    "                        \n",
    "                    mean[cnt_date]=mean[cnt_date]+array_weight[i]*sentim[i]\n",
    "                    cnt_mean[cnt_date]=cnt_mean[cnt_date]+array_weight[i]\n",
    "\n",
    "\n",
    "\n",
    "        weigthed_mean=[None]*array_length\n",
    "\n",
    "        for i in range(len(mean)):\n",
    "            if (mean[i]!=0.0):\n",
    "                weigthed_mean[i]=mean[i]/cnt_mean[i]\n",
    "            else:\n",
    "                weigthed_mean[i]=0\n",
    "                \n",
    "        # save output on file\n",
    "        np.savetxt(mean_path,np.transpose([array_day,array_month,mean,cnt_mean,weigthed_mean]),fmt=\"%d;%d;%s;%s;%s\",delimiter=';',header=\"day;month;sum;weight;mean\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "\n",
    "\n",
    "d={}\n",
    "id_dict={}\n",
    "name_dict={}\n",
    "\n",
    "for file in os.listdir(\"./means\"):\n",
    "    if file.endswith(\".csv\"):\n",
    "        path = r'means/'+file\n",
    "        var = os.path.basename(path)\n",
    "        var= str.split(var,'_got')\n",
    "        var=str.split(var[1],'.')\n",
    "        key = var[0]\n",
    "        d.setdefault(key,[])\n",
    "        d[key]=np.loadtxt(path, comments='++++',delimiter=';',skiprows=1,usecols=(4,))\n",
    "        id_dict.setdefault(key,[])\n",
    "        name_dict.setdefault(key,[])\n",
    "\n",
    "# d, name_dict and id_dict are dictionaries with the same keys as identifiers\n",
    "# For each key, d's values from 0 to 61 are the daily means, value 62 is the # of follower, value 63 is the amount of\n",
    "#unique ids and value 64 in the total amount of tweets.\n",
    "# For each key, id_dict's value 0 is the election result (1 for win, 0 for loss), value 1 is the percentage of votes that\n",
    "# the candidate received and value 2 the pair identifier that pairs up candidates running in the same election.\n",
    "# For each key, name_dict's value 0 is the name of the candidate and value 1 is the last name of the candidate\n",
    "with open('listDeputee.csv', 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=';', quotechar=';')\n",
    "    for row in reader:\n",
    "        if (row[0]!='Id'):\n",
    "            d[row[0]]= np.append(d[row[0]],[float(row[5]),float(row[6]),float(row[7])])\n",
    "            id_dict[row[0]]=np.append(id_dict[row[0]],[float(row[3]),float(row[4]),float(row[8])])\n",
    "            name_dict[row[0]]=np.append(name_dict[row[0]],[row[1],row[2],])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Machine Learning Algorithm\n",
    "\n",
    "We used a linear regression algorithm with multiple outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.600625\n",
      "2 0.5809375\n",
      "3 0.570625\n",
      "4 0.5671875\n",
      "5 0.5728125\n",
      "6 0.561875\n",
      "7 0.5721875\n",
      "8 0.535\n",
      "9 0.5271875\n",
      "10 0.589375\n",
      "11 0.5753125\n",
      "12 0.58125\n",
      "13 0.5734375\n",
      "14 0.5815625\n",
      "15 0.5953125\n",
      "16 0.6096875\n",
      "17 0.6228125\n",
      "18 0.6215625\n",
      "19 0.653125\n",
      "20 0.60625\n",
      "21 0.5840625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/statsmodels/tsa/ar_model.py:735: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.sigma2) + 2 * (1 + self.df_model)/self.nobs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 0.619375\n",
      "23 0.6215625\n",
      "24 0.616875\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import time\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "Y_all_init =[]\n",
    "\n",
    "for keys in id_dict :\n",
    "    Y_all_init=np.append(Y_all_init,id_dict[keys][1])\n",
    "orders = np.array(range(1,25)) #variable to test different order of the AR model\n",
    "for order in orders:\n",
    "    i=0\n",
    "    coeffs = np.zeros([38,order+4])\n",
    "    for keys in d:    \n",
    "        mean = d[keys]\n",
    "        ar_mod = AR(mean[0:61])\n",
    "        ar_res = ar_mod.fit(maxlag = order,method = 'cmle',ic='aic',trend = 'c',tol = 1e-2)\n",
    "        for n in range (len(ar_res.params)):\n",
    "            coeffs[i][n] = ar_res.params[n]\n",
    "        for m in range (3):\n",
    "            coeffs[i][m+order+1] = mean[62+m]\n",
    "        i += 1\n",
    "    shapeMean = np.shape(coeffs)\n",
    "    nbFeature = shapeMean[1]\n",
    "    MEANS_all_init_2 = coeffs\n",
    "    nbRight = 0\n",
    "    nbAll = 0\n",
    "\n",
    "    #Number of time we want to try our prediction with a different set of training and test data.\n",
    "    for iteration in range(400):# 93024\n",
    "        MEANS_all = MEANS_all_init_2\n",
    "        Y_all = Y_all_init\n",
    "\n",
    "        #vector containing the data to test\n",
    "        Y_predict_final = np.zeros((8,1))\n",
    "        MEANS_predict_final = np.zeros((8,nbFeature))\n",
    "        for i in range(8):\n",
    "            #Choose randomly 8 person for the the data test\n",
    "            selected = np.random.randint(0,38-i, 1) #dtype=int)\n",
    "            Y_predict_final[i] = Y_all[selected]\n",
    "            MEANS_predict_final[i] = MEANS_all[selected]\n",
    "\n",
    "            #Supress the data for the test to the data for the training\n",
    "            MEANS_all = np.delete(MEANS_all, selected, 0)\n",
    "            Y_all = np.delete(Y_all, selected, 0)\n",
    "\n",
    "\n",
    "        coef_all = np.zeros((nbFeature))\n",
    "        nbKeep = 0\n",
    "        #We loop on all the data of the training test\n",
    "        for i in range(30):\n",
    "            #Prediction phase - We create the prediction model\n",
    "            clf = linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "            #We remove of the data to the training test\n",
    "            MEANS_fit = np.delete(MEANS_all, i, 0)\n",
    "            Y_fit = np.delete(Y_all, i, 0)\n",
    "\n",
    "            # We fit the data of 29 deputy to the model and we keep 1 for the testing.\n",
    "            clf.fit(MEANS_fit, Y_fit)\n",
    "\n",
    "            #The prediction with the data we reomove\n",
    "            predIt = clf.predict(MEANS_all[i].reshape(1, -1))\n",
    "            #print(predIt)\n",
    "            #If the prediction works, we keep the coeficient of the linear regression. (We add them to an array)\n",
    "            if (Y_all[i] >50 and predIt > 50 ) or (Y_all[i] < 50 and predIt < 50) :\n",
    "                #print('yep')\n",
    "                nbKeep += 1\n",
    "                coef_all += clf.coef_\n",
    "\n",
    "\n",
    "        #The average of all the coeficient we want to keep\n",
    "        coef_all = coef_all/nbKeep\n",
    "\n",
    "        #The preidiction using the average coefficient we compute before.\n",
    "        #We count the number of try we have and the number of succes.\n",
    "        for i in range(len(Y_predict_final)):\n",
    "            nbAll +=1\n",
    "            pred1 = np.dot(MEANS_predict_final[i], coef_all)\n",
    "            if (Y_predict_final[i][0] > 50 and pred1 > 50 ) or (Y_predict_final[i][0] < 50 and pred1 < 50) :\n",
    "                nbRight += 1\n",
    "\n",
    "    print(str(order) + \" \" + str(float(nbRight)/float(nbAll)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5325\n",
      "2 0.62475\n",
      "3 0.52225\n",
      "4 0.51975\n",
      "5 0.574\n",
      "6 0.55625\n",
      "7 0.519\n",
      "8 0.619\n",
      "9 0.537\n",
      "10 0.537\n",
      "11 0.53925\n",
      "12 0.60475\n",
      "13 0.5255\n",
      "14 0.5825\n",
      "15 0.52125\n",
      "16 0.662\n",
      "17 0.65175\n",
      "18 0.58725\n",
      "19 0.6275\n"
     ]
    }
   ],
   "source": [
    "Y_all_init =[]\n",
    "\n",
    "    \n",
    "MEANS_all_init=[]\n",
    "\n",
    "for i in range(1,20):\n",
    "    first=0\n",
    "    first_array = []\n",
    "    first_predict=[]\n",
    "    for keys in id_dict:\n",
    "        if id_dict[keys][2]==i :\n",
    "            if first==0:\n",
    "                first=1\n",
    "                first_array = d[keys]  \n",
    "                first_predict = id_dict[keys][1]\n",
    "            else:\n",
    "                MEANS_all_init=np.append(MEANS_all_init,np.append(first_array,d[keys]))\n",
    "                Y_all_init = np.append(Y_all_init,np.append(first_predict,id_dict[keys][1]))\n",
    "\n",
    "Y_all_init = np.reshape(Y_all_init,[19,2])\n",
    "\n",
    "\n",
    "newmeans = np.reshape(MEANS_all_init,[38,65]) #list of all candidates individualy\n",
    "orders = np.array(range(1,20)) #variable to test different order of the AR model\n",
    "for order in orders:\n",
    "    coeffs = np.zeros([38,order+4]) #initialisation of autoregression coefficients\n",
    "    for i in range(38):    \n",
    "        mean = newmeans[i]\n",
    "        ar_mod = AR(mean[0:61]) # initialisation of the AR model with the mean sentiment values of 1 candidate\n",
    "        ar_res = ar_mod.fit(maxlag = order,method = 'cmle',ic='aic',trend = 'c',tol = 1e-2) #fitting of the AR model\n",
    "        #aic best order = 8, best result 67%\n",
    "        #bic best order = 1, best results 52%\n",
    "        #hqic best order = 2, best result 60%\n",
    "        for n in range (len(ar_res.params)):\n",
    "            coeffs[i][n] = ar_res.params[n] #assignement of the AR model coefficients, with 0 padding in case not max order\n",
    "        for m in range (3):\n",
    "            coeffs[i][m+order+1] = mean[62+m] #appending extra values (followers, total tweets, unique ID)\n",
    "    #AR_coeff = coeffs\n",
    "    AR_coeff = np.reshape(coeffs,[19,2*order+8]) #reshaping coeff to reform pairs win/lose\n",
    "    \n",
    "    \n",
    "    #Prediction testing, By pairs, Using AR model\n",
    "\n",
    "    nbRight = 0 # correct predictions\n",
    "    nbAll = 0 # total predictions\n",
    "    for iteration in range(1000):\n",
    "        MEANS_all = AR_coeff #initialisation\n",
    "        Y_all = Y_all_init\n",
    "\n",
    "        Y_predict_final = np.zeros((4,2))\n",
    "        MEANS_predict_final = np.zeros((4,2*order+8))\n",
    "        \n",
    "        #removing data of some candidates to use as test set\n",
    "        for i in range(4):\n",
    "            selected = np.random.randint(0,19-i, 1)\n",
    "            Y_predict_final[i] = Y_all[selected]\n",
    "            MEANS_predict_final[i] = MEANS_all[selected]\n",
    "\n",
    "            MEANS_all = np.delete(MEANS_all, selected, 0)\n",
    "            Y_all = np.delete(Y_all, selected, 0)\n",
    "\n",
    "\n",
    "        #print(Y_predict_final)\n",
    "\n",
    "        coef_all = np.zeros((2,2*order+8))\n",
    "        nbKeep = 0\n",
    "        \n",
    "        #Training linear regression model using leave-one-out technique\n",
    "        for i in range(15):\n",
    "            #Prediction phase - We create the prediction model\n",
    "            clf = linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "            MEANS_fit = np.delete(MEANS_all, i, 0)\n",
    "            Y_fit = np.delete(Y_all, i, 0)\n",
    "\n",
    "            # We fit the data of 18 deputy to the model and we keep 1 for the testing.\n",
    "            clf.fit(MEANS_fit, Y_fit)\n",
    "\n",
    "            #print (np.shape(clf.coef_))\n",
    "            predIt = clf.predict(MEANS_all[i].reshape(1, -1))\n",
    "            #print (predIt, '  to be : ', Y_all[i])\n",
    "\n",
    "            if (Y_all[i][0] > Y_all[i][1] and predIt[0][0] > predIt[0][1] ) or (Y_all[i][0] < Y_all[i][1] and predIt[0][0] < predIt[0][1]) :\n",
    "                #print('yep')\n",
    "                nbKeep += 1\n",
    "                coef_all += clf.coef_ #keeping only coefficients that result in a correct prediction\n",
    "            #else :\n",
    "                #print('nope')\n",
    "        #averaging the coefficients that gave a correct prediction\n",
    "        coef_all = coef_all/nbKeep\n",
    "        \n",
    "        #testing the linear regression model on test set data\n",
    "        for i in range(len(Y_predict_final)):\n",
    "            nbAll +=1\n",
    "            pred1 = np.dot(MEANS_predict_final[i],coef_all[0])\n",
    "            pred2 = np.dot(MEANS_predict_final[i], coef_all[1])\n",
    "            if (Y_predict_final[i][0] > Y_predict_final[i][1] and pred1 > pred2 ) or (Y_predict_final[i][0] < Y_predict_final[i][1] and pred1 < pred2) :\n",
    "                nbRight += 1\n",
    "\n",
    "    print(str(order) + \" \" + str(float(nbRight)/float(nbAll)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Conclusions\n",
    "\n",
    "Sentiment Analysis, like any other natural language processing tool, is hard to perform and does not give extremely accurate results. Performing it on Tweets is tricky because many slang words are used and often the analysis does not output anything. We were aware of this but saw that with a big dataset we were able to have a reasonable number of tweets that could be processed by the sentiment analysis and give accurate results.\n",
    "\n",
    "Another issue that we encountered was that the number of tweets differed a lot depending on the popularity of the candidate. However, since we looked at big elections in a big country, we were able to have a quite big dataset anyways.\n",
    "\n",
    "used one election to keep same condition\n",
    "\n",
    "For further improvements of this work, a bigger dataset would be helpful to train more the machine learning algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
